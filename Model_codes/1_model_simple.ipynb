{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_image(path):\n",
    "    images_files = os.listdir(path)\n",
    "    dir_name = os.path.basename(path)\n",
    "    for i, image in enumerate(images_files):\n",
    "        new_name = f\"{dir_name}-{i+1}.jpg\"\n",
    "        old_path = os.path.join(path, image)\n",
    "        new_path = os.path.join(path, new_name)\n",
    "        os.rename(old_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keraton', 'Sido']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name = \"../Dataset/simple_dataset/jenis\"\n",
    "os.listdir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153 images belonging to 2 classes.\n",
      "Found 38 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator( rescale=1/255.0,\n",
    "                                   validation_split = 0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(dir_name, \n",
    "                                                    target_size = (150, 150),\n",
    "                                                    batch_size = 10, \n",
    "                                                    subset='training')\n",
    "val_generator = train_datagen.flow_from_directory(dir_name, \n",
    "                                                  target_size = (150, 150),\n",
    "                                                  batch_size = 10, \n",
    "                                                  subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(), \n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 17, 17, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18496)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               9470464   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,494,561\n",
      "Trainable params: 9,494,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "16/16 - 4s - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 4s/epoch - 279ms/step\n",
      "Epoch 2/15\n",
      "16/16 - 4s - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 4s/epoch - 255ms/step\n",
      "Epoch 3/15\n",
      "16/16 - 4s - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 4s/epoch - 243ms/step\n",
      "Epoch 4/15\n",
      "16/16 - 4s - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 4s/epoch - 267ms/step\n",
      "Epoch 5/15\n",
      "16/16 - 5s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 5s/epoch - 292ms/step\n",
      "Epoch 6/15\n",
      "16/16 - 5s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 5s/epoch - 305ms/step\n",
      "Epoch 7/15\n",
      "16/16 - 4s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 4s/epoch - 274ms/step\n",
      "Epoch 8/15\n",
      "16/16 - 5s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 5s/epoch - 301ms/step\n",
      "Epoch 9/15\n",
      "16/16 - 4s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 4s/epoch - 281ms/step\n",
      "Epoch 10/15\n",
      "16/16 - 5s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 5s/epoch - 312ms/step\n",
      "Epoch 11/15\n",
      "16/16 - 5s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 5s/epoch - 295ms/step\n",
      "Epoch 12/15\n",
      "16/16 - 5s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 - 5s/epoch - 299ms/step\n",
      "Epoch 13/15\n",
      "16/16 - 5s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 5s/epoch - 284ms/step\n",
      "Epoch 14/15\n",
      "16/16 - 6s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 6s/epoch - 366ms/step\n",
      "Epoch 15/15\n",
      "16/16 - 4s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 4s/epoch - 261ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=15,\n",
    "            validation_data=val_generator,\n",
    "            verbose=2\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77 images belonging to 2 classes.\n",
      "Found 19 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "dir_name = '../Dataset/simple_dataset/motif'\n",
    "\n",
    "train_datagen_v2 = ImageDataGenerator( rescale=1/255.0,\n",
    "                                   validation_split = 0.2,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "train_generator_2 = train_datagen_v2.flow_from_directory(dir_name, \n",
    "                                                    target_size = (150, 150),\n",
    "                                                    batch_size = 20, \n",
    "                                                    subset='training')\n",
    "val_generator_2 = train_datagen_v2.flow_from_directory(dir_name, \n",
    "                                                  target_size = (150, 150),\n",
    "                                                  batch_size = 20, \n",
    "                                                  subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "4/4 - 3s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 3s/epoch - 809ms/step\n",
      "Epoch 2/15\n",
      "4/4 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 591ms/step\n",
      "Epoch 3/15\n",
      "4/4 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 575ms/step\n",
      "Epoch 4/15\n",
      "4/4 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 608ms/step\n",
      "Epoch 5/15\n",
      "4/4 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 594ms/step\n",
      "Epoch 6/15\n",
      "4/4 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 607ms/step\n",
      "Epoch 7/15\n",
      "4/4 - 3s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 3s/epoch - 650ms/step\n",
      "Epoch 8/15\n",
      "4/4 - 3s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 3s/epoch - 635ms/step\n",
      "Epoch 9/15\n",
      "4/4 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 601ms/step\n",
      "Epoch 10/15\n",
      "4/4 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 607ms/step\n",
      "Epoch 11/15\n",
      "4/4 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 609ms/step\n",
      "Epoch 12/15\n",
      "4/4 - 2s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 2s/epoch - 592ms/step\n",
      "Epoch 13/15\n",
      "4/4 - 3s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 3s/epoch - 648ms/step\n",
      "Epoch 14/15\n",
      "4/4 - 3s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 3s/epoch - 653ms/step\n",
      "Epoch 15/15\n",
      "4/4 - 3s - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 3s/epoch - 673ms/step\n"
     ]
    }
   ],
   "source": [
    "history_2 = model.fit(\n",
    "            train_generator_2,\n",
    "            epochs=15,\n",
    "            validation_data=val_generator_2,\n",
    "            verbose=2\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
